{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values removed from 'execution_time_ssd' and 'execution_time_hdd' columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('2000_test.csv')\n",
    "\n",
    "# Remove values from the specified columns\n",
    "df['execution_time_ssd'] = None\n",
    "df['execution_time_hdd'] = None\n",
    "\n",
    "# Save the modified dataframe back to CSV\n",
    "df.to_csv('2000_test_modified.csv', index=False)\n",
    "\n",
    "print(\"Values removed from 'execution_time_ssd' and 'execution_time_hdd' columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Get these arguments from ML Model\n",
    "# new_dataset_size = 146      # Size of the new dataset GB\n",
    "# default_dataset_size = 146  # Default dataset size  GB\n",
    "# avg_execution_time_on_cache = 247  # Average Execution Time of the application from main_dataset.csv (History)  (s)\n",
    "# waiting_time = 0  # Waiting time\n",
    "# # total_execution_time = 300  # Total execution time on cache\n",
    "# disk_util = 0.981         # average disk utilization during training  execution time\n",
    "# disk_time = 1         # training execution time / total possible execution time of workload execution time\n",
    "# normalized_io_rate = disk_util * disk_util   # Normalized I/O rate\n",
    "# local_ssd_bw = 66.5  # Local SSD bandwidth (From 1000 Run Monitoring)  (Mb/s)\n",
    "# hdd_bw = 36         # HDD bandwidth (From 1000 Run Monitoring)  (Mb/s)\n",
    "# second_partition_bw = local_ssd_bw # Assumed that we want execution time on HDD\n",
    "# avg_cache_hit = 0.0545  # Average cache hit rate  \n",
    "# avg_io_rate_on_first_partition = 36.03\n",
    "\n",
    "# time_on_first_partition = (new_dataset_size / default_dataset_size) * avg_execution_time_on_cache\n",
    "# print(time_on_first_partition)\n",
    "# total_time_on_first_partition = waiting_time + time_on_first_partition\n",
    "# print(total_time_on_first_partition)\n",
    "# normalized_io_time_on_first_partition = total_time_on_first_partition * normalized_io_rate\n",
    "# print(normalized_io_time_on_first_partition)\n",
    "# compute_time = time_on_first_partition - normalized_io_time_on_first_partition\n",
    "# # avg_io_rate_on_first_partition = (avg_cache_hit * local_ssd_bw) + ((1 - avg_cache_hit) * hdd_bw)      # TODO: regression needed\n",
    "# print (\"avg_io_rate on first partition\", avg_io_rate_on_first_partition)\n",
    "# io_time_on_second_partition = (avg_io_rate_on_first_partition / second_partition_bw) * normalized_io_time_on_first_partition\n",
    "# print(\"io time on second partition\", io_time_on_second_partition)\n",
    "# total_time_on_second_partition = compute_time + io_time_on_second_partition\n",
    "\n",
    "# print(\"Time on cache:\", time_on_first_partition)\n",
    "# print(\"Total time on cache:\", total_time_on_first_partition)\n",
    "# print(\"Normalized I/O time on cache:\", normalized_io_time_on_first_partition)\n",
    "# print(\"Compute time:\", compute_time)\n",
    "# print(\"Avg I/O rate on cache:\", avg_io_rate_on_first_partition)\n",
    "# print(\"I/O time on HDD:\", io_time_on_second_partition)\n",
    "# print(\"Total time on HDD:\", total_time_on_second_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created variables: ['Application \\nName', 'Framework', 'Application\\nLink', 'Log Path on\\nServer', 'Dataset Size', 'Cache Size', '# epoch', 'Type', 'I/O rate', 'Compute_time', 'Execution Time\\n on SSD real', 'SSD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc)', 'SSD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL)', 'Average Error \\nRate on SSD', 'SSD calc\\n from HDD', 'Error Rate \\n(SSD from HDD)', 'Average Error \\nRate on SSD from HDD', 'Execution Time\\non Cache real', 'Execution Time\\n on SAN_HDD real', 'HDD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc).1', 'HDD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL).1', 'Average Error\\nRate on HDD', 'HDD calc time\\nfrom ssd', 'Error Rate \\r\\n(HDD from SSD)', 'Average Error \\nRate on HDD from SSD', 'SSD BW(MB/s)', 'SAN_HDD BW(MB/s)', 'Cache BW(MB/s)', 'CacheBW_Calc', 'Hit Ratio', 'GPU_Util\\nCache', 'GPU_Util\\nSSD', 'GPU_Util\\nHDD', 'Disk_Util_cache', 'Disk_Util_ssd', 'Disk_Util_hdd', 'CPU_Util_cache', 'CPU_Util_ssd', 'CPU_Util_hdd', 'Read Sequential', 'Machine']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Formulation Evaluation.csv')\n",
    "\n",
    "# Get all column names\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Create variables for each column\n",
    "for column in column_names:\n",
    "    # Use globals() to create variables dynamically\n",
    "    globals()[column] = df[column].tolist()\n",
    "\n",
    "# Now you can access each column as a separate variable\n",
    "# For example, if you have columns 'A', 'B', and 'C':\n",
    "# print(A)  # This will print the contents of column 'A'\n",
    "# print(B)  # This will print the contents of column 'B'\n",
    "# print(C)  # This will print the contents of column 'C'\n",
    "\n",
    "# To see all created variables\n",
    "print(\"Created variables:\", column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved CPU intensive formulations to ./cpu_intensive_formulations.csv\n",
      "Saved High hit formulations to ./high_hit_formulations.csv\n",
      "Saved Low hit formulations to ./low_hit_formulations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('Formulation Evaluation.csv')\n",
    "\n",
    "grouped = df.groupby('Type')\n",
    "\n",
    "output_dir = './'\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for name, group in grouped:\n",
    "    filename = f\"{name.replace(' ', '_').lower()}_formulations.csv\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    group.to_csv(filepath, index=False)\n",
    "    print(f\"Saved {name} formulations to {filepath}\")\n",
    "\n",
    "# print(\"Checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_execution_times(row):\n",
    "    # Constants (you may need to adjust these based on your specific requirements)\n",
    "    SSD_FACTOR = 1.2  # SSD is typically faster than cache\n",
    "    HDD_FACTOR = 2.5  # HDD is typically slower than cache\n",
    "\n",
    "    # Calculate execution times based on cache time\n",
    "    execution_time_cache = row['execution_time_cache']\n",
    "    execution_time_ssd = execution_time_cache / SSD_FACTOR\n",
    "    execution_time_hdd = execution_time_cache * HDD_FACTOR\n",
    "\n",
    "    # Created variables: ['Application \\nName', 'Framework', 'Application\\nLink', \n",
    "    # 'Log Path on\\nServer', 'Dataset Size', 'Cache Size', '# epoch', 'Type', 'I/O rate', \n",
    "    # 'Compute_time', 'Execution Time\\n on SSD real', 'SSD_cacl_time\\nfrom cache (our calc)',\n",
    "    #  'Error Rate (our calc)', 'SSD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL)', \n",
    "    # 'Average Error \\nRate on SSD', 'SSD calc\\n from HDD', 'Error Rate \\n(SSD from HDD)',\n",
    "    #  'Average Error \\nRate on SSD from HDD', 'Execution Time\\non Cache real', \n",
    "    # 'Execution Time\\n on SAN_HDD real', 'HDD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc).1',\n",
    "    #  'HDD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL).1', 'Average Error\\nRate on HDD', \n",
    "    # 'HDD calc time\\nfrom ssd', 'Error Rate \\r\\n(HDD from SSD)', 'Average Error \\nRate on HDD from SSD',\n",
    "    #  'SSD BW(MB/s)', 'SAN_HDD BW(MB/s)', 'Cache BW(MB/s)', 'CacheBW_Calc', 'Hit Ratio', 'GPU_Util\\nCache',\n",
    "    #  'GPU_Util\\nSSD', 'GPU_Util\\nHDD', 'Disk_Util_cache', 'Disk_Util_ssd', 'Disk_Util_hdd', 'CPU_Util_cache',\n",
    "    #  'CPU_Util_ssd', 'CPU_Util_hdd', 'Read Sequential', 'Machine']\n",
    "\n",
    "    # TODO: Get these arguments from ML Model\n",
    "    new_dataset_size = 146      # Size of the new dataset GB\n",
    "    default_dataset_size = 146  # Default dataset size  GB\n",
    "    avg_execution_time_on_cache = execution_time_cache  # Average Execution Time of the application from main_dataset.csv (History)  (s)\n",
    "    waiting_time = 0  # Waiting time\n",
    "    # total_execution_time = 300  # Total execution time on cache\n",
    "    disk_util = row['Disk_Util_cache']         # average disk utilization during training  execution time\n",
    "    disk_time = 1         # training execution time / total possible execution time of workload execution time\n",
    "    normalized_io_rate = disk_util * disk_util   # Normalized I/O rate\n",
    "    local_ssd_bw = row['SSD BW(MB/s)']  # Local SSD bandwidth (From 1000 Run Monitoring)  (Mb/s)\n",
    "    hdd_bw = row['SAN_HDD BW(MB/s)']    # HDD bandwidth (From 1000 Run Monitoring)  (Mb/s)\n",
    "    second_partition_bw = local_ssd_bw # Assumed that we want execution time on HDD\n",
    "    avg_cache_hit = row['Hit Ratio']   # Average cache hit rate  \n",
    "    avg_io_rate_on_first_partition = row['Cache BW(MB/s)']\n",
    "\n",
    "    time_on_first_partition = (new_dataset_size / default_dataset_size) * avg_execution_time_on_cache\n",
    "    print(time_on_first_partition)\n",
    "    total_time_on_first_partition = waiting_time + time_on_first_partition\n",
    "    print(total_time_on_first_partition)\n",
    "    normalized_io_time_on_first_partition = total_time_on_first_partition * normalized_io_rate\n",
    "    print(normalized_io_time_on_first_partition)\n",
    "    compute_time = time_on_first_partition - normalized_io_time_on_first_partition\n",
    "    # avg_io_rate_on_first_partition = (avg_cache_hit * local_ssd_bw) + ((1 - avg_cache_hit) * hdd_bw)      # TODO: regression needed\n",
    "    print (\"avg_io_rate on first partition\", avg_io_rate_on_first_partition)\n",
    "    io_time_on_second_partition = (avg_io_rate_on_first_partition / second_partition_bw) * normalized_io_time_on_first_partition\n",
    "    print(\"io time on second partition\", io_time_on_second_partition)\n",
    "    total_time_on_second_partition = compute_time + io_time_on_second_partition\n",
    "\n",
    "    # Update the row with new values\n",
    "    row['execution_time_ssd'] = execution_time_ssd\n",
    "    row['execution_time_hdd'] = execution_time_hdd\n",
    "    return row\n",
    "\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('2000_test_modified.csv')\n",
    "\n",
    "# Apply the calculation to each row\n",
    "df = df.apply(calculate_execution_times, axis=1)\n",
    "\n",
    "# Round the values to integers (assuming execution times are in whole seconds)\n",
    "df['execution_time_ssd'] = df['execution_time_ssd'].round().astype(int)\n",
    "df['execution_time_hdd'] = df['execution_time_hdd'].round().astype(int)\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "df.to_csv('2000_test_calculated.csv', index=False)\n",
    "\n",
    "print(\"Calculations complete. Results saved to 2000_test_calculated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating cpu_intensive.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created variables: ['Application \\nName', 'Framework', 'Application\\nLink', 'Log Path on\\nServer', 'Dataset Name', 'Dataset Size', 'Cache Size', '# epoch', 'Type', 'I/O rate', 'Compute_time', 'Execution Time\\n on SSD real', 'SSD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc)', 'SSD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL)', 'Average Error \\nRate on SSD', 'SSD calc\\n from HDD', 'Error Rate \\n(SSD from HDD)', 'Average Error \\nRate on SSD from HDD', 'Execution Time\\non Cache real', 'Execution Time\\n on SAN_HDD real', 'HDD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc).1', 'HDD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL).1', 'Average Error\\nRate on HDD', 'HDD calc time\\nfrom ssd', 'Error Rate \\r\\n(HDD from SSD)', 'Average Error \\nRate on HDD from SSD', 'SSD BW(MB/s)', 'SAN_HDD BW(MB/s)', 'Cache BW(MB/s)', 'CacheBW_Calc', 'Hit Ratio', 'GPU_Util\\nCache', 'GPU_Util\\nSSD', 'GPU_Util\\nHDD', 'Disk_Util_cache', 'Disk_Util_ssd', 'Disk_Util_hdd', 'CPU_Util_cache', 'CPU_Util_ssd', 'CPU_Util_hdd', 'Read Sequential', 'Machine']\n",
      "cpu_intensive.csv DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['dataset_name'] = 'NotFamous'\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['submission_time'] = 1\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\3783224168.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Read the cpu_intensive_formulation.csv file\n",
    "df = pd.read_csv('cpu_intensive_formulations.csv')\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Create variables for each column\n",
    "for column in column_names:\n",
    "    # Use globals() to create variables dynamically\n",
    "    globals()[column] = df[column].tolist()\n",
    "\n",
    "# Now you can access each column as a separate variable\n",
    "# For example, if you have columns 'A', 'B', and 'C':\n",
    "# print(A)  # This will print the contents of column 'A'\n",
    "# print(B)  # This will print the contents of column 'B'\n",
    "# print(C)  # This will print the contents of column 'C'\n",
    "\n",
    "# To see all created variables\n",
    "print(\"Created variables:\", column_names)\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = [\n",
    "    'I/O rate',\n",
    "    'Execution Time\\n on SAN_HDD real',\n",
    "    'Execution Time\\n on SSD real',\n",
    "    'Execution Time\\non Cache real',\n",
    "    'Application \\nName'\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the required columns\n",
    "new_df = df[columns_to_keep]\n",
    "\n",
    "# Rename the columns to match the format in scale_pro.py\n",
    "new_df.columns = [\n",
    "    'io_rate',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_cache',\n",
    "    'application_name'\n",
    "]\n",
    "\n",
    "# Add new columns\n",
    "new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
    "new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
    "new_df['dataset_name'] = 'NotFamous'\n",
    "new_df['submission_time'] = 1\n",
    "new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
    "new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
    "new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
    "new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n",
    "\n",
    "\n",
    "# Reorder columns to match the desired output\n",
    "column_order = [\n",
    "    'id',\n",
    "    'submission_time',\n",
    "    'partition',\n",
    "    'application_name',\n",
    "    'dataset_name',\n",
    "    'read_io',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_cache'\n",
    "]\n",
    "\n",
    "new_df = new_df[column_order]\n",
    "\n",
    "new_df.to_csv('cpu_intensive.csv', index=True)\n",
    "\n",
    "print(\"cpu_intensive.csv DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating famous_dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created variables: ['Application \\nName', 'Framework', 'Application\\nLink', 'Log Path on\\nServer', 'Dataset Name', 'Dataset Size', 'Cache Size', '# epoch', 'Type', 'I/O rate', 'Compute_time', 'Execution Time\\n on SSD real', 'SSD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc)', 'SSD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL)', 'Average Error \\nRate on SSD', 'SSD calc\\n from HDD', 'Error Rate \\n(SSD from HDD)', 'Average Error \\nRate on SSD from HDD', 'Execution Time\\non Cache real', 'Execution Time\\n on SAN_HDD real', 'HDD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc).1', 'HDD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL).1', 'Average Error\\nRate on HDD', 'HDD calc time\\nfrom ssd', 'Error Rate \\r\\n(HDD from SSD)', 'Average Error \\nRate on HDD from SSD', 'SSD BW(MB/s)', 'SAN_HDD BW(MB/s)', 'Cache BW(MB/s)', 'CacheBW_Calc', 'Hit Ratio', 'GPU_Util\\nCache', 'GPU_Util\\nSSD', 'GPU_Util\\nHDD', 'Disk_Util_cache', 'Disk_Util_ssd', 'Disk_Util_hdd', 'CPU_Util_cache', 'CPU_Util_ssd', 'CPU_Util_hdd', 'Read Sequential', 'Machine']\n",
      "famous_dataset.csv DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['submission_time'] = 1\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\2072982908.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Read the cpu_intensive_formulation.csv file\n",
    "df = pd.read_csv('low_hit_formulations.csv')\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Create variables for each column\n",
    "for column in column_names:\n",
    "    # Use globals() to create variables dynamically\n",
    "    globals()[column] = df[column].tolist()\n",
    "\n",
    "# Now you can access each column as a separate variable\n",
    "# For example, if you have columns 'A', 'B', and 'C':\n",
    "# print(A)  # This will print the contents of column 'A'\n",
    "# print(B)  # This will print the contents of column 'B'\n",
    "# print(C)  # This will print the contents of column 'C'\n",
    "\n",
    "# To see all created variables\n",
    "print(\"Created variables:\", column_names)\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = [\n",
    "    'I/O rate',\n",
    "    'Execution Time\\n on SAN_HDD real',\n",
    "    'Execution Time\\n on SSD real',\n",
    "    'Execution Time\\non Cache real',\n",
    "    'Application \\nName',\n",
    "    'Dataset Name'\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the required columns\n",
    "new_df = df[columns_to_keep]\n",
    "\n",
    "# Rename the columns to match the format in scale_pro.py\n",
    "new_df.columns = [\n",
    "    'io_rate',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_cache',\n",
    "    'application_name',\n",
    "    'dataset_name'\n",
    "]\n",
    "\n",
    "# Add new columns\n",
    "new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
    "new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
    "# new_df['dataset_name'] = 'NotFamous'\n",
    "new_df['submission_time'] = 1\n",
    "new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
    "new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
    "new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
    "new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n",
    "\n",
    "\n",
    "# Reorder columns to match the desired output\n",
    "column_order = [\n",
    "    'id',\n",
    "    'submission_time',\n",
    "    'partition',\n",
    "    'application_name',\n",
    "    'dataset_name',\n",
    "    'read_io',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_cache'\n",
    "]\n",
    "\n",
    "new_df = new_df[column_order]\n",
    "\n",
    "new_df.to_csv('famous_dataset.csv', index=True)\n",
    "\n",
    "print(\"famous_dataset.csv DONE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating high_hit.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created variables: ['Application \\nName', 'Framework', 'Application\\nLink', 'Log Path on\\nServer', 'Dataset Name', 'Dataset Size', 'Cache Size', '# epoch', 'Type', 'I/O rate', 'Compute_time', 'Execution Time\\n on SSD real', 'SSD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc)', 'SSD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL)', 'Average Error \\nRate on SSD', 'SSD calc\\n from HDD', 'Error Rate \\n(SSD from HDD)', 'Average Error \\nRate on SSD from HDD', 'Execution Time\\non Cache real', 'Execution Time\\n on SAN_HDD real', 'HDD_cacl_time\\nfrom cache (our calc)', 'Error Rate (our calc).1', 'HDD_cacl_time\\nfrom cache (REAL)', 'Error Rate (REAL).1', 'Average Error\\nRate on HDD', 'HDD calc time\\nfrom ssd', 'Error Rate \\n(HDD from SSD)', 'Average Error \\nRate on HDD from SSD', 'SSD BW(MB/s)', 'SAN_HDD BW(MB/s)', 'Cache BW(MB/s)', 'CacheBW_Calc', 'Hit Ratio', 'GPU_Util\\nCache', 'GPU_Util\\nSSD', 'GPU_Util\\nHDD', 'Disk_Util_cache', 'Disk_Util_ssd', 'Disk_Util_hdd', 'CPU_Util_cache', 'CPU_Util_ssd', 'CPU_Util_hdd', 'Read Sequential', 'Machine']\n",
      "famous_dataset.csv DONE!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['dataset_name'] = 'NotFamous'\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['submission_time'] = 1\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
      "C:\\Users\\amirs\\AppData\\Local\\Temp\\ipykernel_21112\\1808492790.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Read the cpu_intensive_formulation.csv file\n",
    "df = pd.read_csv('high_hit_formulations.csv')\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Create variables for each column\n",
    "for column in column_names:\n",
    "    # Use globals() to create variables dynamically\n",
    "    globals()[column] = df[column].tolist()\n",
    "\n",
    "# Now you can access each column as a separate variable\n",
    "# For example, if you have columns 'A', 'B', and 'C':\n",
    "# print(A)  # This will print the contents of column 'A'\n",
    "# print(B)  # This will print the contents of column 'B'\n",
    "# print(C)  # This will print the contents of column 'C'\n",
    "\n",
    "# To see all created variables\n",
    "print(\"Created variables:\", column_names)\n",
    "\n",
    "# Select only the required columns\n",
    "columns_to_keep = [\n",
    "    'I/O rate',\n",
    "    'Execution Time\\n on SAN_HDD real',\n",
    "    'Execution Time\\n on SSD real',\n",
    "    'Execution Time\\non Cache real',\n",
    "    'Application \\nName'\n",
    "]\n",
    "\n",
    "# Create a new dataframe with only the required columns\n",
    "new_df = df[columns_to_keep]\n",
    "\n",
    "# Rename the columns to match the format in scale_pro.py\n",
    "new_df.columns = [\n",
    "    'io_rate',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_cache',\n",
    "    'application_name'\n",
    "]\n",
    "\n",
    "# Add new columns\n",
    "new_df['id'] = [random.randint(1, 20) for _ in range(len(new_df))]\n",
    "new_df['partition'] = [random.choice(['cache_with_hdd', 'central_hdd']) for _ in range(len(new_df))]\n",
    "new_df['dataset_name'] = 'NotFamous'\n",
    "new_df['submission_time'] = 1\n",
    "new_df['read_io'] = new_df['io_rate']  # Assuming io_rate is equivalent to read_io\n",
    "new_df['execution_time_ssd'] = new_df['execution_time_ssd'].astype(int)\n",
    "new_df['execution_time_hdd'] = new_df['execution_time_hdd'].astype(int)\n",
    "new_df['execution_time_cache'] = new_df['execution_time_cache'].astype(int)\n",
    "\n",
    "\n",
    "# Reorder columns to match the desired output\n",
    "column_order = [\n",
    "    'id',\n",
    "    'submission_time',\n",
    "    'partition',\n",
    "    'application_name',\n",
    "    'dataset_name',\n",
    "    'read_io',\n",
    "    'execution_time_ssd',\n",
    "    'execution_time_hdd',\n",
    "    'execution_time_cache'\n",
    "]\n",
    "\n",
    "new_df = new_df[column_order]\n",
    "\n",
    "new_df.to_csv('high_hit.csv', index=True)\n",
    "\n",
    "print(\"famous_dataset.csv DONE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
